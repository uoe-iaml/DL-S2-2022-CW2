{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<p style='text-align: right;'>Ver. 1.0</p>**\n",
    "\n",
    "# Introductory Applied Machine Learning (IAML) Coursework 2 - Semester 2, 2021-22\n",
    "\n",
    "### Author: Hiroshi Shimodaira and Shuzhuang Xu\n",
    "\n",
    "## Important Instructions\n",
    "\n",
    "#### It is important that you follow the instructions below carefully for things to work properly.\n",
    "\n",
    "You need to set up and activate your environment as you would do for your labs, see Learn section on Labs.  **You will need to use Noteable to create one of the files you will submit (the PDF)**.  Do **NOT** create the PDF in some other way, we will not be able to mark it.  If you want to develop your answers in your own environment, you should make sure you are using the same packages we are using, by running the cell which does imports below.\n",
    "\n",
    "Read the instructions in this notebook carefully, especially where asked to name variables with a specific name. Wherever you are required to produce code you should use code cells, otherwise you should use markdown cells to report results and explain answers. In most cases we indicate the nature of answer we are expecting (code/text), and also provide the required code/markdown cell.\n",
    "\n",
    "- We will use the IAML Learn page for any announcements, updates, and FAQs on this assignment. Please ***visit the page frequently*** to find the latest information/changes.\n",
    "- Data files that you will be using are included in the [git repository](https://github.com/uoe-iaml/DL-S2-2022-CW2) for this coursework.\n",
    "- There is a helper file 'iaml22cw2_helpers.py' in the git repository, which you should upload to your environment.\n",
    "- Some of the topics in this coursework are covered in weeks 7 and 8 of the course. Focus first on questions on topics that you have covered already, and come back to the other questions as the lectures progress.\n",
    "- Keep your answers brief and concise.\n",
    "- Make sure to show all your code/working.\n",
    "- All the figures you present should have axis labels, titles, and grid lines unless specified explicitly. If you think grid lines spoiling readability, you can adjust the line width and/or line style. Figures should not be too small to read.\n",
    "- Write readable code. While we do not expect you to follow PEP8 to the letter, the code should be adequately understandable, with plots/visualisations correctly labelled. Do use inline comments when doing something non-standard.\n",
    "- When asked to present numerical values, make sure to represent real numbers in the appropriate precision to exemplify your answer. \n",
    "- When you use libraries specified in this coursework, you should use the default parameters unless specified explicitly.\n",
    "- The criteria on which you will be judged include the quality of the textual answers and/or any plots asked for. For higher marks, when asked you need to give good and concise discussions based on experiments and theories using your own words.\n",
    "\n",
    "- You will see <html>\\\\pagebreak</html> at the start of each subquestion.  ***Do not remove these, if you do we will not be able to mark your coursework.***\n",
    "\n",
    "#### Good Scholarly Practice\n",
    "Please remember the University requirement regarding all assessed work for credit. Details about this can be found at:\n",
    "http://web.inf.ed.ac.uk/infweb/admin/policies/academic-misconduct\n",
    "\n",
    "Specifically, this assignment should be your own individual work. We will employ tools for detecting misconduct.\n",
    "\n",
    "Moreover, please note that Piazza is NOT a forum for discussing the solutions of the assignment. You may ask private questions. You can use the office hours to ask questions.\n",
    "\n",
    "### SUBMISSION Mechanics\n",
    "This assignment will account for 30% of your final mark. We ask you to submit answers to all questions.\n",
    "\n",
    "You will submit (1) a PDF of your Notebook via Gradescope, and (2) the Notebook itself via Learn.  Your grade will be based on the PDF, we will only use the Notebook if we need to see details.  **You must use the following procedure to create the materials to submit**.\n",
    "\n",
    "1. Make sure your Notebook and the datasets are in Noteable and will run.  If you developed your answers in Noteable, this is already done.\n",
    "\n",
    "2. Select **Kernel->Restart & Run All** to create a clean copy of your submission, this will run the cells in order from top to bottom.  This may take a while (a few hours) to complete, ensure that all the output and plots have complete before you proceed.\n",
    "\n",
    "3. Select **File->Download as->PDF via LaTeX (.pdf)** and wait for the PDF to be created and downloaded.\n",
    "\n",
    "4. Select **File->Download as->Notebook (.ipynb)**\n",
    "\n",
    "5. You now should have in your download folder the pdf and the notebook.  Rename them sNNNNNNN.pdf and sNNNNNNN.ipynb, where sNNNNNNN is your matriculation number (student number).\n",
    "\n",
    "**Details on submission instructions will be announced and documented on Learn well before the deadline**. \n",
    "\n",
    "The submission deadline for this assignment is **5th April 2022 at 16:00 UK time (UTC)**.  Don't leave it to the last minute!\n",
    "\n",
    "\n",
    "#### IMPORTS\n",
    "Execute the cell below to import all packages you will be using for this assignment.  If you are not using Noteable, make sure the python and package version numbers reported match the python and package numbers specified in the comment at the end of this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import sklearn\n",
    "import scipy\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from iaml22cw2_helpers import *\n",
    "print_versions();\n",
    "\n",
    "# You can add other libraries here if necessary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1  Analysis and classification of a music genre data set\n",
    "\n",
    "#### 55 marks out of 110 for this coursework\n",
    "\n",
    "In this question, we use a modified version of the [GTZAN Dataset - Music Genre Classification](https://www.kaggle.com/andradaolteanu/gtzan-dataset-music-genre-classification). The data set is a collections of music of 10 genres and each genre has 100 music samples of parameterised features.\n",
    "<!-- that have been parameterised from the original audio signals.\n",
    "the GTZAN data set (http://marsyas.info). The original data set is provided as audio signals, but\n",
    " -->\n",
    "\n",
    "\n",
    "Load the data set in the following manner:\n",
    "```python\n",
    "        Xtrn, Ytrn, Xtst, Ytst = pickle.loads(Path('dset_q1.pkl').read_bytes())\n",
    "```\n",
    "**Xtrn** and **Xtst** contain feature vectors for training and testing, respectively, where rows correspond to music instances and columns to features (attributes) of 513 dimensions.\n",
    "**Ytrn** and **Ytst** contain the corresponding genre labels, where genre  name (i.e. class) is coded in an integer between 0 and 9.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== Question 1.1 --- [9 marks] ==========\n",
    "\n",
    "We want to see how each feature distributes for different classes. Instead of displaying the distributions of all the 513 dimensions, we are going to pick up several ones only in this question.\n",
    "\n",
    "1. [Code] Using **Xtrn**, for each of the classes, 0,1,2, plot a histogram of each of dimensions, 0,2,4,6,8 and place the resultant 15 histograms in a grid of 3-by-5, where you show the histograms left-to-right then top-to-bottom, so that rows correspond to classes and columns to dimensions.\n",
    "\n",
    "  Note that:\n",
    "  - Add a title to each histogram to clarify the class and dimension.\n",
    "  - Share the same axis ranges among the histograms of the same dimension, which helps to compare histograms between different classes.\n",
    "  \n",
    "  \n",
    "2. [Text] Are there any differences in term of dimension and class? Explain your findings briefly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak\n",
    "## Your answers for Question 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) ***Your answer goes here***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== Question 1.2 --- [8 marks] =========\n",
    "\n",
    "In this question, we consider dimensionality reduction of the music data set with Principal Component Analysis (PCA) to visualise the data.\n",
    "At first, apply PCA to **Xtrn** using Sklearn's **PCA** with default parameters, where you pass **Xtrn** to the function without applying preprocessing.\n",
    "\n",
    "1. [Code] Plot the following two graphs side-by-side. \n",
    "     - A graph of the variance explained by a principal component as a function of principal component number, where you depict the graph for up to the first 200 principal components only. Use a linear or log scale for the y-axis.\n",
    "     - A graph of the cumulative explained variance ratio as a function of principal component number, where you depict the graph for up to the first 200 principal components only. Use a linear scale for the y-axis.\n",
    "\n",
    "\n",
    "2. [Code] Mapping all the instances in **Xtrn** on to the 2D space spanned with the first two principal components, plot a scatter graph of the instances on the space, where different colours or symbols should be used to denote different genres. The x-axis should correspond to the first principal component, and the y-axis to the second component. Use a legend.\n",
    "\n",
    "\n",
    "3. [Text] Discuss your findings based on the graphs (results) you obtained, comparing the graphs for training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak\n",
    "## Your answers for Question 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(2) Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) ***Your answer goes here***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== Question 1.3 --- [5 marks] =========\n",
    "\n",
    "We now standardise the data by mean and standard deviation using the method described below, and look into how the standardisation has impacts on PCA.\n",
    "\n",
    "Create the standardised training data **Xtrn_s** and test data **Xtst_s** in your code in the following manner.\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(Xtrn)\n",
    "Xtrn_s = scaler.transform(Xtrn) # standardised training data\n",
    "Xtst_s = scaler.transform(Xtst) # standardised test data\n",
    "```\n",
    "\n",
    "1. [Code] Using **Xtrn_s** instead of **Xtrn**, answer the subquestion 1.2.1.\n",
    "\n",
    "2. [Code] Using **Xtrn_s** instead of **Xtrn**, answer the subquestion 1.2.2.\n",
    "\n",
    "3. [Text] Comparing with the results you obtained in 1.2, explain and discuss your findings briefly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak\n",
    "## Your answers for Question 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(2) Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) ***Your answer goes here***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== Question 1.4 --- [6 marks] ==========\n",
    "\n",
    "We now want to investigate the effect of the data standardisation on the classification with multi-class logistic regression.\n",
    "\n",
    "1. [Code] Do the following for each of the original data set and the standardised data set.\n",
    "  1. Train a logistic regression classifier using the training set.\n",
    "  2. Run a classification experiment on each of the training and test sets and report the classification accuracy and a confusion matrix for each set. \n",
    "  \n",
    "  Note that:\n",
    "  - Use Sklearn's **LogisticRegression** with parameters 'max_iter=1000' and 'random_state=0' while use default values for the other parameters. \n",
    "  - Your confusion matrix should show the numbers of samples in figures instead of ratios.\n",
    "  \n",
    "\n",
    "2. [Text] Comparing the results obtained, explain and describe your findings briefly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak\n",
    "## Your answers for Question 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) ***Your answer goes here***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== Question 1.5 --- [12 marks] ==========\n",
    "\n",
    "We now want to run experiments on Support Vector Machines (SVMs) with an RBF kernel, where we try to optimise the penalty parameter $C$. By using 3-fold CV on the standardised training data **Xtrn_s** described above, estimate the classification accuracy, while you vary the penalty parameter $C$ in the range 0.01 to 1000 - use 11 values spaced equally in log space, where the logarithm base is 10. Use Sklearn’s **SVC** and **StratifiedKFold** with default parameters unless specified. Do not shuffle the data.\n",
    "Answer the following questions.\n",
    "\n",
    "1. [Code] Calculate the mean and standard deviation of cross-validation classification accuracy for each $C$, and plot them against $C$ by using a log-scale for the x-axis, where standard deviations are shown with error bars. On the same figure, plot the same information (i.e. the mean and standard deviation of classification accuracy) for the training set in the cross validation.\n",
    "\n",
    "\n",
    "2. [Text] Comment (in brief) on any observations.\n",
    "\n",
    "\n",
    "3. [Text] Report the highest mean cross-validation accuracy and the value of $C$ which yielded it.\n",
    "\n",
    "\n",
    "4. [Code] Using the best parameter value you found, evaluate the corresponding best classifier on the test set { **Xtst_s**, **Ytst** }. Report the number of instances correctly classified and classification accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak\n",
    "## Your answers for Question 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) ***Your answer goes here***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) ***Your answer goes here***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4) Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== Question 1.6 --- [15 marks] \n",
    "\n",
    "We would like to visualise the decision regions for the logistic regression classifier we trained in Question 1.4 and the SVM in Question 1.5 (see the notes below for detailed conditions), where we use the models trained with the standardised data **Xtrn_s**.\n",
    "Since it is not possible to visualise the original vector of many dimensions, we consider a two-dimensional plane, i.e. cross section, in the original space and visualise decisions regions for the points on that plane, ignoring points outside the plane. We here employ the plane spanned by the first two principal components, where we assume that the plane shares the same origin as the one of the original vector space. \n",
    "\n",
    "1. [Code] For each of the classifiers, plot the decisions regions for the rectangular area of 200-by-200 grid points whose left-upper corner is $(−5\\sigma_1, 5\\sigma_2)$ and right-bottom corner is $(5\\sigma_1, −5\\sigma_2)$, where $\\sigma_1$ and $\\sigma_2$ denote the standard deviations for the first principal component and second principal component, respectively. \n",
    "Use Matplotlib's **contourf**, where you specify \"cmap='jet', levels=np.arange(-0.5,10)\".\n",
    "You should also use Matplotlib's **colorbar** to display a colour bar, where you specify \"ticks=range(10)\".\n",
    "\n",
    "\n",
    "2. [Text] Explain and discuss your findings briefly.\n",
    "\n",
    "Notes:\n",
    "- For SVM, use $C=1$ irrespective of the optimal value you obtained in Question 1.5.\n",
    "- You should have two separate figures, one for the logistic regression, and the other for the SVM.\n",
    "\n",
    "***Hint***: The projected point $\\mathbf{z} = (z_1,...,z_D)$ with PCA is given as $\\mathbf{z} = xV^T$, where $V$ is a $D$-by-$D$ square matrix, whose $i$-th row corresponds to the $i$-th eigenvector. Note that both\n",
    "$\\mathbf{z}$ and $\\mathbf{x}$ represent the same point with different coordinate systems, the original vector space and the one spanned with $D$ eigenvectors, and the two coordinate systems share the same origin. Any points on the 2D-plane spanned with the first two principal components can be expressed as $\\mathbf{z} = (z_1, z_2, 0, . . . , 0)$. You can find the corresponding $\\mathbf{x}$ in the original vector space by $\\mathbf{x} = \\mathbf{z}V$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak\n",
    "## Your answers for Question 1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) ***Your answer goes here***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--# Question 2 Multivariate Gaussian distributions and binary classification-->\n",
    "# Question 2 Generative models and parameter inference\n",
    "\n",
    "#### 55 marks out of 110 for this coursework\n",
    "\n",
    "\n",
    "In this question, we consider multivariate Gaussian distributions. We generate samples from a distribution, estimate the parameters of the model (i.e. distribution). We then carry out classification experiments on the data of two classes to find how a limited number of training samples has an impact on the classification. We also consider Gaussian mixture models (GMMs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== Question 2.1 --- [5 marks] ==========\n",
    "\n",
    "Consider a two-dimensional (2D) Gaussian distribution with the mean vector $\\mathbf{\\mu}=(0,0)$.\n",
    "\n",
    "[Code] For each of the covariance matrices, $\\Sigma_1, \\Sigma_2, \\Sigma_3$, shown below, do the following:\n",
    "- Plot 2D contours of the distribution and a 3D surface of the distribution side-by-side.\n",
    "- Display the theoretical maximum value of the distribution at the top of the figure of 2D contours.\n",
    "\n",
    "Note that:\n",
    "- You should display a separate set of figures for each covariance matrix.\n",
    "- For plotting 2D contours, use Matplotlib's **contour** with default parameters, in which the x-axis corresponds to $x_1$ and the y-axis to $x_2$, where $\\mathbf{x} = (x_1,x_2)$. Use the fixed axis limits of $[-4,4,-4,4]$, and use \"scaled\" option for axis().\n",
    "- For plotting a 3D surface, use Matplotlib's **plot_surface**, where use the 'coolwarm' colormap.\n",
    "- The covariance matrices are as follows.\n",
    "$$\n",
    " \\Sigma_1 = \\left(\\begin{array}{cc} 1 & 0\\\\ 0 & 1 \\end{array}\\right),\n",
    " \\quad\n",
    "  \\Sigma_2 = \\left(\\begin{array}{cc} 0.1 & 0\\\\ 0 & 0.1 \\end{array}\\right),\n",
    " \\quad\n",
    "  \\Sigma_3 = \\left(\\begin{array}{cc} 3 & -1.5\\\\ -1.5 & 1 \\end{array}\\right)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak\n",
    "## Your answer for Question 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== Question 2.2 --- [10 marks] ==========\n",
    "\n",
    "We here consider the parameter estimation of a 2D Gaussian distribution from samples, for which we generate $N$ samples from a model (i.e. a 2D Gaussian distribution), estimate the model parameters from the generated samples, and evaluate the accuracy of the parameter estimation.\n",
    "As an evaluation measure, we use the function **kl_mvn()**, which is provided in 'iaml21cw2_helpers.py'.\n",
    "The function calculates dissimilarity from a Gaussian distribution $P$ to another Gaussian distribution $Q$ based on \n",
    "the Kullback-Leibler (KL) divergence. \n",
    "Note that details of the KL divergence is beyond the scope of this course and you are not required to study it in this coursework.\n",
    "\n",
    "We now consider a 2D Gaussian distribution with a mean vector $\\mathbf{\\mu}=(0,0)$ and a covariance matrix $\\Sigma = \\left(\\begin{array}{cc}1 &0.5\\\\0.5&1\\end{array}\\right)$.\n",
    "\n",
    "1. [Code]\n",
    "  - Set the random seed by calling np.random.seed(1). This is important to make the experiment reproducible.\n",
    "  - Do the following 6 times\n",
    "     - Generate $N=10$ samples from the model using Numpy's **multivariate_normal** and estimate the model parameters (i.e. mean vector and covariance matrix) from the samples using the maximum-likelihood estimation.\n",
    "     - Plot the samples.\n",
    "     - On the same figure as the one for plotting samples, plot a 2D contour for each of the true distribution and estimated distribution. To avoid clutter, you plot a ***single contour*** whose level is equal to 0.022 for each distribution. See the note below regarding the line styles to use.\n",
    "     - Obtain the KL divergence from the true distribution to the estimated one and display it above the figure.\n",
    "\n",
    "  Note that\n",
    "  \n",
    "  - Use the same axis limits for all the six figures, and use equal scaling for the x and y axes to make circles circular.\n",
    "  - The 6 figures you obtained should be shown in a 2-by-3 grid as shown below (no need to show lines between/around figures):\\\n",
    "  \\------------------------\\\n",
    "  | Fig-1 | Fig-2 | Fig-3 |\\\n",
    "  | Fig-4 | Fig-5 | Fig-6 |\\\n",
    "  \\------------------------\n",
    "  - When plotting a 2D contour of a distribution, use a dashed line for the true distribution and a solid line for the estimated distribution.\n",
    "\n",
    "\n",
    "2. [Code] Letting $N=20$ instead of 10, do the same as 1 above.\n",
    "\n",
    "\n",
    "3. [Text] Based on the results you obtained in 1 and 2, explain your findings and give discussions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak\n",
    "## Your answers for Question 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(2) Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) ***Your answer goes here***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== Question 2.3 --- [8 marks] ==========\n",
    "\n",
    "We now look into the relationship between the accuracy of parameter estimation and the number of samples for estimation.\n",
    "As a true model, we use a 2D Gaussian distribution whose parameters are the same as the ones used in Question 2.2.\n",
    "\n",
    "1. [Code] \n",
    "  - For $N=15, 30, 60, 100, 500, 2000$, do the following.\n",
    "    - Set the random seed by calling np.random.seed(1).\n",
    "    - Repeat the following 10 times:\n",
    "      - Generate $N$ samples from the model.\n",
    "      - Estimate the model parameters from the samples using the Maximum Likelihood estimation.\n",
    "      - Obtain the KL divergence from the true distribution to the estimated distribution.\n",
    "    - Calculate the mean and standard deviation of the KL divergence over the 10 repetitions above.\n",
    "  - Plot the mean KL divergence as a function of $N$, where standard deviations are shown with error bars. Use a logarithmic scale for the x-axis.\n",
    "\n",
    "\n",
    "2. [Text] Explain your findings and give discussions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak\n",
    "## Your answers for Question 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) ***Your answer goes here***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== Question 2.4 --- [18 marks] ==========\n",
    "\n",
    "Here we consider binary classification of 2D data with (i) logistic regression, (ii) linear SVM, (iii) non-linear SVM with a polynomial kernel, and (iv) multivariate Gaussian Bayes classifier. We are particularly interested in how the decision boundaries and classification performance (e.g. accuracy) change as the number of training samples increases.\n",
    "\n",
    "At first, load the data set in the following manner:\n",
    "```python\n",
    "    Xtrn,Ytrn,Xtst,Ytst = pickle.loads(Path('dset_q2_4.pkl').read_bytes())\n",
    "```\n",
    "where **Xtrn**,**Ytrn**,**Xtst**,**Ytst** are the data and labels for training and those for testing, respectively. **Xtrn** is a numpy nd-array with a shape $(L,2)$, where $L$ denotes the number of samples and 2 denotes the dimensions of features for a sample. A label takes a binary value - 0 for Class 0 (denoted as $C_0$) and 1 for Class 1 ($C_1$).\n",
    "The samples of each class were generated based on a 2D Gaussian distribution with a prior probability $P(C_k)$, where\n",
    "$P(C_0) = \\frac{2}{3}, \\; P(C_1)=\\frac{1}{3}$.\n",
    "The 2D Gaussian distribution for $C_k$ is given as follows.\n",
    "$$\n",
    "    p(\\mathbf{x}|C_k) = \\frac{1}{2\\pi|\\Sigma_k|^{1/2}}\\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\mathbf{\\mu}_k)\\Sigma_k^{-1}(\\mathbf{x}-\\mathbf{\\mu}_k)^T\\right),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "   \\mathbf{\\mu}_0 = (3,5),\\; \\Sigma_0 = \\left(\\begin{array}{cc}1 & -0.5\\\\-0.5 & 2\\end{array}\\right), \\quad\n",
    "   \\mathbf{\\mu}_1 = (8,2),\\; \\Sigma_1 = \\left(\\begin{array}{cc}4 & 0.4\\\\0.4 & 2\\end{array}\\right). \n",
    "   $$\n",
    "\n",
    "1. [Code]\n",
    "  For $N = 15,30,50,100,200,500$, do the following:\n",
    "  1. Train each of the four classifiers using the first $N$ samples from the training set {**Xtrn**,**Ytrn**}.\n",
    "  2. For each class, plot a 2D contour for each of the true and estimated distributions on a figure in the same way as we did in Question 2.2. Use the equal scaling for the x and y axes so that circles are shown circular.\n",
    "  3. Plot the $N$ samples on the same figure, using different colours for different classes.\n",
    "  4. On the same figure, plot the theoretical decision boundary. (see the notes below)\n",
    "  5. On the same figure, plot the decision boundary that is formed by each of the four classifiers. (see the notes below)\n",
    "  6. Using the whole test data {**Xtst**,**Ytst**}, obtain the classification accuracy (Acc) and F1 score for each of the classifiers (i), (ii), and (iii), and display the information to the right of the figure. (NB: there is no need to do this for the classifier (iv).)\n",
    "  \n",
    "  *** Notes ***\n",
    "  - Theoretical decision boundary: it is the decision boundary that is formed with the Bayes classification rule when the true model parameters are known. Since we assume multivariate Gaussian distributions in this question, the boundary is given using the the multivariate Gaussian Bayes classifier (shown below) with the true model parameters instead of the estimated ones. When plotting the theoretical decision boundary, use a dashed line in 'black'.\n",
    "  - Multivariate Gaussian Bayes classifier: we employ the Bayes classification rule, $\\arg\\max_k P(C_k) p(\\mathbf{x}|C_k)$, where $P(C_k)$ is the prior probability of $C_k$, $k=0,1$, and $p(\\mathbf{x}|C_k)$ follows a multivariate Gaussian distribution. When plotting the decision boundary, use a solid line in 'black'.\n",
    " - Logistic regression: use the default parameters. When plotting a decision boundary, use a solid line in 'red'\n",
    "  - Linear SVM: use a linear kernel. For plotting, use a solid line in 'green'.\n",
    "  - Non-linear SVM with a polynomial kernel: use a polynomial kernel with a degree of 4. For plotting, use a solid line in 'magenta'.\n",
    "  - When you plot decision boundaries with Matplotlib's **contour**, use at least 100 grid points for each of the x and y axis.\n",
    "\n",
    "\n",
    "2. [Text] Explain your findings and give discussions, referring to underfitting and overfitting problems of classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak\n",
    "## Your answers for Question 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) ***Your answer goes here***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== Question 2.5 --- [14 marks] ==========\n",
    "\n",
    "This is a mini project, in which you infer model parameters from a limited number of observations or samples generated from an unknown model. To simplify the problem, we assume that the model is a Gaussian Mixture Model (GMM), but we do not know its parameters,\n",
    "i.e., the number of mixture components, $K$, the mean vector, $\\mathbf{\\mu}_k$, the covariance matrix, $\\Sigma_k$, and the weight of the mixture component, $p_k$, for $k=1,\\ldots,$K.\n",
    "\n",
    "Given a set of observations, your task is to identify the GMM, i.e. find the most probable values of the parameters. It should be noted that the values you estimated would vary depending on the set of observations you use. In addition, the training algorithm used for the k-means clustering and GMMs does not guarantee global optima.\n",
    "You should demonstrate that the parameters you obtained are optimal/suboptimal or reasonable on a certain ground.\n",
    "\n",
    "As you recall, a GMM is defined as follows:\n",
    "$$\n",
    " p(\\mathbf{x}) = \\sum_{k=1}^K p_k {\\cal N}(\\mathbf{x}|\\mathbf{\\mu}_k,\\Sigma_k)\n",
    "$$\n",
    "where\n",
    "$$\n",
    " {\\cal N}(\\mathbf{x}|\\mathbf{\\mu}_k,\\Sigma_k) = \\frac{1}{(2\\pi)^{D/2}|\\Sigma_k|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\mathbf{\\mu}_k)\\Sigma_k^{-1}(\\mathbf{x}-\\mathbf{\\mu}_k)^T\\right),\n",
    "$$\n",
    "$p_k$ is the weight of the component, $k$, and $D$ is the dimensions of features. $D=2$ in this task.\n",
    "\n",
    "Load the data set in the following manner:\n",
    "```python\n",
    "        X = pickle.loads(Path('dset_q2_5.pkl').read_bytes())\n",
    "```\n",
    "We use this **X** as a set of observations to estimate the parameters.\n",
    "\n",
    "\n",
    "1. [Code] Before we estimate the parameters of the GMM, we employ the k-means clustering for a basic analysis. Using Sklearn's  **KMeans** with the option \"random_state=0\", applying the k-means clustering to the data set **X** and plot the sum-squared error as a function of $k$, where you use $k=2,3,\\ldots,30$.\n",
    "\n",
    "\n",
    "2. [Text] Briefly comment on the result you obtained in 1 above. \n",
    "\n",
    "\n",
    "3. [Text] Describe how you would identify the GMM given the data set **X**. \n",
    "\n",
    "\n",
    "4. [Code] Using Sklearn's **GaussianMixture**, implement your idea/method that you described in 3 above, run experiments, and show the results. You should display a table that shows the parameters of the GMM you obtained, where each row corresponds to the mixture component number, $k$ and columns to $p_k$, $\\mathbf{\\mu}_k$, and $\\Sigma_k$.\n",
    "\n",
    "\n",
    "5. [Code] Plot a 2D contour of the estimated GMM, where you plot a contour whose level is equal to 0.005 only. \n",
    "\n",
    "\n",
    "6. [Text] Describe and discuss how confident you are about the parameters you obtained.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak\n",
    "## Your answer for Question 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) ***Your answer goes here***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) ***Your answer goes here***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4) Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (5) Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6) ***Your answer goes here***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell's output will confirm all cells have been run if you select Kernel->Restart & Run All.\n",
    "# Wait until you see the output printed\n",
    "print(\"*****************************\")\n",
    "print(\"*                           *\")\n",
    "print(\"* All cells have been run!! *\")\n",
    "print(\"*                           *\")\n",
    "print(\"*****************************\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
